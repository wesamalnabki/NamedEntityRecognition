{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450cf615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=Eh3ckSO0YMZW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d98a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas \n",
    "# ! pip install scikit-learn\n",
    "# ! pip install seqeval==0.0.12\n",
    "# ! pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57827cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Juan</td>\n",
       "      <td>B-PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vive</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>España</td>\n",
       "      <td>B-GPE-LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentence_idx    word             tag\n",
       "0           0             0    Juan        B-PERSON\n",
       "1           1             0    vive               O\n",
       "2           2             0      en               O\n",
       "3           3             0  España  B-GPE-LOCATION\n",
       "4           4             0       .               O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('./NER_DS_SPANISH/spanish_ner_dataset_INCIBE.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c7fca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48791"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df.sentence_idx.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa57333a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353854, 45707)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset:\n",
    "split_thresh = df['sentence_idx'].max() * 0.9\n",
    "df_train, df_valid = df[df['sentence_idx'] < split_thresh], df[df['sentence_idx'] >= split_thresh]\n",
    "len(df_train), len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85008fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = lambda s: [ [w,t] for w,t in zip(s[\"word\"].values.tolist(),s[\"tag\"].values.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97216cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_grouped = df_train.groupby(\"sentence_idx\").apply(agg_func)\n",
    "x_valid_grouped = df_valid.groupby(\"sentence_idx\").apply(agg_func)\n",
    "\n",
    "x_train_sentences = [[s[0] for s in sent] for sent in x_train_grouped.values]\n",
    "x_valid_sentences = [[s[0] for s in sent] for sent in x_valid_grouped.values]\n",
    "\n",
    "x_train_tags = [[t[1] for t in tag] for tag in x_train_grouped.values]\n",
    "x_valid_tags = [[t[1] for t in tag] for tag in x_valid_grouped.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f5ed977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list = df['tag'].unique()\n",
    "label_map = {label: i for i, label in enumerate(tag_list)}\n",
    "label_map_inv = {i: label for i, label in enumerate(tag_list)}\n",
    "num_labels = len(tag_list) + 1\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1a3a631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags: 37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O                 299007\n",
       "I-PRODUCTS         35923\n",
       "B-PRODUCTS         16242\n",
       "I-ORGANIZATION      7190\n",
       "B-NUMBERS_C         6936\n",
       "I-NUMBERS_C         5874\n",
       "B-ORGANIZATION      5636\n",
       "I-GPE-LOCATION      4772\n",
       "B-GPE-LOCATION      3039\n",
       "I-DATE              2550\n",
       "B-DATE              1619\n",
       "I-QUANTITIES        1073\n",
       "I-FACILITIES        1062\n",
       "B-QUANTITIES         969\n",
       "I-PERSON             938\n",
       "I-ART                932\n",
       "B-PERSON             856\n",
       "I-EVENTS             658\n",
       "I-LAW                655\n",
       "I-TIME               540\n",
       "I-MONEY              500\n",
       "B-TIME               477\n",
       "B-FACILITIES         374\n",
       "B-ART                295\n",
       "B-MONEY              286\n",
       "I-LOCATION           219\n",
       "B-EVENTS             216\n",
       "B-LOCATION           177\n",
       "B-LANGUAGE           165\n",
       "B-NORP               164\n",
       "B-LAW                103\n",
       "B-NUMBERS_O           49\n",
       "I-LANGUAGE            24\n",
       "I-GROUP               19\n",
       "I-NORP                10\n",
       "B-GROUP                8\n",
       "I-NUMBERS_O            4\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of tags: {}\".format(len(df.tag.unique())))\n",
    "frequencies = df.tag.value_counts()\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2bab6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRODUCTS', 52165),\n",
       " ('ORGANIZATION', 12826),\n",
       " ('NUMBERS_C', 12810),\n",
       " ('GPE-LOCATION', 7811),\n",
       " ('DATE', 4169),\n",
       " ('QUANTITIES', 2042),\n",
       " ('PERSON', 1794),\n",
       " ('FACILITIES', 1436),\n",
       " ('ART', 1227),\n",
       " ('TIME', 1017),\n",
       " ('EVENTS', 874),\n",
       " ('MONEY', 786),\n",
       " ('LAW', 758),\n",
       " ('LOCATION', 396),\n",
       " ('LANGUAGE', 189),\n",
       " ('NORP', 174),\n",
       " ('NUMBERS_O', 53),\n",
       " ('GROUP', 27)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {}\n",
    "for tag, count in zip(frequencies.index, frequencies):\n",
    "    if tag != \"O\":\n",
    "        if tag[2:] not in tags.keys():\n",
    "            tags[tag[2:]] = count\n",
    "        else:\n",
    "            tags[tag[2:]] += count\n",
    "    continue\n",
    "\n",
    "sorted(tags.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb584a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Juan</td>\n",
       "      <td>B-PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vive</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>España</td>\n",
       "      <td>B-GPE-LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentence_idx    word             tag\n",
       "0           0             0    Juan        B-PERSON\n",
       "1           1             0    vive               O\n",
       "2           2             0      en               O\n",
       "3           3             0  España  B-GPE-LOCATION\n",
       "4           4             0       .               O"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_to_remove = [\"B-TIME\", \"I-TIME\",\n",
    "                      \"B-NUMBERS_O\", \"I-NUMBERS_O\",\n",
    "                      \"B-NORP\", \"I-NORP\",\n",
    "                      \"B-LAW\", \"I-LAW\",\n",
    "                      \"B-NUMBERS_C\", \"I-NUMBERS_C\",\n",
    "                      \"B-DATE\", \"I-DATE\",\n",
    "                      \"B-ART\", \"I-ART\", \n",
    "                     ]\n",
    "df = df[~df.tag.isin(entities_to_remove)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e3cb31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags: 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O                 299007\n",
       "I-PRODUCTS         35923\n",
       "B-PRODUCTS         16242\n",
       "I-ORGANIZATION      7190\n",
       "B-ORGANIZATION      5636\n",
       "I-GPE-LOCATION      4772\n",
       "B-GPE-LOCATION      3039\n",
       "I-QUANTITIES        1073\n",
       "I-FACILITIES        1062\n",
       "B-QUANTITIES         969\n",
       "I-PERSON             938\n",
       "B-PERSON             856\n",
       "I-EVENTS             658\n",
       "I-MONEY              500\n",
       "B-FACILITIES         374\n",
       "B-MONEY              286\n",
       "I-LOCATION           219\n",
       "B-EVENTS             216\n",
       "B-LOCATION           177\n",
       "B-LANGUAGE           165\n",
       "I-LANGUAGE            24\n",
       "I-GROUP               19\n",
       "B-GROUP                8\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of tags: {}\".format(len(df.tag.unique())))\n",
    "frequencies = df.tag.value_counts()\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eca1c4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list = df['tag'].unique()\n",
    "label_map = {label: i for i, label in enumerate(tag_list)}\n",
    "label_map_inv = {i: label for i, label in enumerate(tag_list)}\n",
    "num_labels = len(tag_list) + 1\n",
    "(num_labels/2)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "485e0ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRODUCTS', 52165),\n",
       " ('ORGANIZATION', 12826),\n",
       " ('GPE-LOCATION', 7811),\n",
       " ('QUANTITIES', 2042),\n",
       " ('PERSON', 1794),\n",
       " ('FACILITIES', 1436),\n",
       " ('EVENTS', 874),\n",
       " ('MONEY', 786),\n",
       " ('LOCATION', 396),\n",
       " ('LANGUAGE', 189),\n",
       " ('GROUP', 27)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {}\n",
    "for tag, count in zip(frequencies.index, frequencies):\n",
    "    if tag != \"O\":\n",
    "        if tag[2:] not in tags.keys():\n",
    "            tags[tag[2:]] = count\n",
    "        else:\n",
    "            tags[tag[2:]] += count\n",
    "    continue\n",
    "\n",
    "sorted(tags.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec32257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80346"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tags.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0517856d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-PERSON': 0,\n",
       " 'O': 1,\n",
       " 'B-GPE-LOCATION': 2,\n",
       " 'I-PERSON': 3,\n",
       " 'B-ORGANIZATION': 4,\n",
       " 'I-ORGANIZATION': 5,\n",
       " 'B-MONEY': 6,\n",
       " 'I-GPE-LOCATION': 7,\n",
       " 'B-LANGUAGE': 8,\n",
       " 'B-PRODUCTS': 9,\n",
       " 'I-PRODUCTS': 10,\n",
       " 'B-FACILITIES': 11,\n",
       " 'I-FACILITIES': 12,\n",
       " 'B-EVENTS': 13,\n",
       " 'I-EVENTS': 14,\n",
       " 'B-GROUP': 15,\n",
       " 'I-GROUP': 16,\n",
       " 'I-MONEY': 17,\n",
       " 'B-QUANTITIES': 18,\n",
       " 'I-QUANTITIES': 19,\n",
       " 'B-LOCATION': 20,\n",
       " 'I-LOCATION': 21,\n",
       " 'I-LANGUAGE': 22}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids = {k: v for v, k in enumerate(df.tag.unique())}\n",
    "ids_to_labels = {v: k for v, k in enumerate(df.tag.unique())}\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bfa6bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Juan</td>\n",
       "      <td>B-PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vive</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>España</td>\n",
       "      <td>B-GPE-LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentence_idx    word             tag\n",
       "0           0             0    Juan        B-PERSON\n",
       "1           1             0    vive               O\n",
       "2           2             0      en               O\n",
       "3           3             0  España  B-GPE-LOCATION\n",
       "4           4             0       .               O"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas has a very handy \"forward fill\" function to fill missing values based on the last upper non-nan value\n",
    "df = df.fillna(method='ffill')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "635dc44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Juan</td>\n",
       "      <td>B-PERSON</td>\n",
       "      <td>Juan vive en España .</td>\n",
       "      <td>B-PERSON,O,O,B-GPE-LOCATION,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vive</td>\n",
       "      <td>O</td>\n",
       "      <td>Juan vive en España .</td>\n",
       "      <td>B-PERSON,O,O,B-GPE-LOCATION,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>O</td>\n",
       "      <td>Juan vive en España .</td>\n",
       "      <td>B-PERSON,O,O,B-GPE-LOCATION,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>España</td>\n",
       "      <td>B-GPE-LOCATION</td>\n",
       "      <td>Juan vive en España .</td>\n",
       "      <td>B-PERSON,O,O,B-GPE-LOCATION,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>Juan vive en España .</td>\n",
       "      <td>B-PERSON,O,O,B-GPE-LOCATION,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentence_idx    word             tag               sentence  \\\n",
       "0           0             0    Juan        B-PERSON  Juan vive en España .   \n",
       "1           1             0    vive               O  Juan vive en España .   \n",
       "2           2             0      en               O  Juan vive en España .   \n",
       "3           3             0  España  B-GPE-LOCATION  Juan vive en España .   \n",
       "4           4             0       .               O  Juan vive en España .   \n",
       "\n",
       "                     word_labels  \n",
       "0  B-PERSON,O,O,B-GPE-LOCATION,O  \n",
       "1  B-PERSON,O,O,B-GPE-LOCATION,O  \n",
       "2  B-PERSON,O,O,B-GPE-LOCATION,O  \n",
       "3  B-PERSON,O,O,B-GPE-LOCATION,O  \n",
       "4  B-PERSON,O,O,B-GPE-LOCATION,O  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create a new column called \"sentence\" which groups the words by sentence \n",
    "df['sentence'] = df[['sentence_idx','word','tag']].groupby(['sentence_idx'])['word'].transform(lambda x: ' '.join(x))\n",
    "# let's also create a new column called \"word_labels\" which groups the tags by sentence \n",
    "df['word_labels'] = df[['sentence_idx','word','tag']].groupby(['sentence_idx'])['tag'].transform(lambda x: ','.join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ad486c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Juan vive en España .</td>\n",
       "      <td>B-PERSON,O,O,B-GPE-LOCATION,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prueba de etiquetado</td>\n",
       "      <td>O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pedro Sánchez Pérez - Castejón , presidente de...</td>\n",
       "      <td>B-PERSON,I-PERSON,I-PERSON,I-PERSON,I-PERSON,O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fotocenter.es – Imprimir tus mejores recuerdos...</td>\n",
       "      <td>B-ORGANIZATION,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip to content</td>\n",
       "      <td>O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0                              Juan vive en España .   \n",
       "1                               Prueba de etiquetado   \n",
       "2  Pedro Sánchez Pérez - Castejón , presidente de...   \n",
       "3  fotocenter.es – Imprimir tus mejores recuerdos...   \n",
       "4                                    Skip to content   \n",
       "\n",
       "                                         word_labels  \n",
       "0                      B-PERSON,O,O,B-GPE-LOCATION,O  \n",
       "1                                              O,O,O  \n",
       "2  B-PERSON,I-PERSON,I-PERSON,I-PERSON,I-PERSON,O...  \n",
       "3                   B-ORGANIZATION,O,O,O,O,O,O,O,O,O  \n",
       "4                                              O,O,O  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55014191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hidden split chat\n",
    "df['sentence'] = df['sentence'].str.replace('\\\\xa0','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1a49bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['sentence'] = df['sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba5635a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33544"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de510fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da8a6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "                                              do_lower_case=False)# bert-base-multilingual-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6618493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        sentence = self.data.sentence[index].strip().split()  \n",
    "        word_labels = self.data.word_labels[index].split(\",\") \n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                             is_split_into_words=True, \n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [labels_to_ids[label] for label in word_labels] \n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        \n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "          if mapping[0] == 0 and mapping[1] != 0:\n",
    "            # overwrite label\n",
    "            encoded_labels[idx] = labels[i]\n",
    "            i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "408d4244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (33544, 2)\n",
      "TRAIN Dataset: (26835, 2)\n",
      "TEST Dataset: (6709, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = df.sample(frac=train_size,random_state=200)\n",
    "test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "717a5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./NER_DS_SPANISH/test_set_sent.txt','w', encoding='utf-8') as wrt:\n",
    "    wrt.write('\\n'.join(test_dataset.sentence.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a472628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.iloc[13518].word_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dce899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[80][\"input_ids\"]), training_set[80][\"labels\"]):\n",
    "    # print('{0:10}  {1}'.format(token, label))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d7933b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1682c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
    "\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cb1b57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27438d1507c04f28bcaf07f920f9feda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))\n",
    "# model.to(device)\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\",# \"bert-base-multilingual-uncased\", \n",
    "                                                   num_labels=len(labels_to_ids), \n",
    "                                                   ignore_mismatched_sizes=True)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2256380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = training_set[0]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)\n",
    "# labels = labels.type(torch.LongTensor)\n",
    "\n",
    "input_ids = input_ids.to(device, dtype = torch.long)\n",
    "attention_mask = attention_mask.to(device, dtype = torch.long)\n",
    "labels = labels.to(device, dtype = torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f80fe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9567, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "809c141d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 23])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0e585f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9f750ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(tk, ml, epk):    \n",
    "\n",
    "    directory = \"./beto_es_cased\"\n",
    "    \n",
    "    p  = os.path.join(directory,f\"epoch_{str(epk)}\")\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    \n",
    "    # save vocabulary of the tokenizer\n",
    "    tk.save_vocabulary(p)\n",
    "    \n",
    "    # save the model weights and its configuration file\n",
    "    ml.save_pretrained(p)\n",
    "    print('All files saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3889b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 3.061131477355957, ID: 0\n",
      "Training loss per 100 training steps: 1.1223724056588542, ID: 100\n",
      "Training loss per 100 training steps: 0.8686017387393695, ID: 200\n",
      "Training loss per 100 training steps: 0.749726538469031, ID: 300\n",
      "Training loss per 100 training steps: 0.6695765233656712, ID: 400\n",
      "Training loss per 100 training steps: 0.6147739472287798, ID: 500\n",
      "Training loss per 100 training steps: 0.5736479909250671, ID: 600\n",
      "Training loss per 100 training steps: 0.5342271324863574, ID: 700\n",
      "Training loss per 100 training steps: 0.513883754387777, ID: 800\n",
      "Training loss per 100 training steps: 0.4938842484326444, ID: 900\n",
      "Training loss per 100 training steps: 0.47428736023020585, ID: 1000\n",
      "Training loss per 100 training steps: 0.45876516616707846, ID: 1100\n",
      "Training loss per 100 training steps: 0.44713820607093574, ID: 1200\n",
      "Training loss per 100 training steps: 0.43483819077500097, ID: 1300\n",
      "Training loss per 100 training steps: 0.42391242151169223, ID: 1400\n",
      "Training loss per 100 training steps: 0.4152422711069508, ID: 1500\n",
      "Training loss per 100 training steps: 0.40581450510832906, ID: 1600\n",
      "Training loss per 100 training steps: 0.3983168177266136, ID: 1700\n",
      "Training loss per 100 training steps: 0.3927293400916745, ID: 1800\n",
      "Training loss per 100 training steps: 0.385523578833528, ID: 1900\n",
      "Training loss per 100 training steps: 0.3808245696406672, ID: 2000\n",
      "Training loss per 100 training steps: 0.37558376531904364, ID: 2100\n",
      "Training loss per 100 training steps: 0.3703242643714723, ID: 2200\n",
      "Training loss per 100 training steps: 0.364821538387408, ID: 2300\n",
      "Training loss per 100 training steps: 0.3612555831419409, ID: 2400\n",
      "Training loss per 100 training steps: 0.3569329426560949, ID: 2500\n",
      "Training loss per 100 training steps: 0.3536729838556593, ID: 2600\n",
      "Training loss per 100 training steps: 0.35004341852206766, ID: 2700\n",
      "Training loss per 100 training steps: 0.3457513241611371, ID: 2800\n",
      "Training loss per 100 training steps: 0.3414251976561409, ID: 2900\n",
      "Training loss per 100 training steps: 0.3372074965602026, ID: 3000\n",
      "Training loss per 100 training steps: 0.3348273621418269, ID: 3100\n",
      "Training loss per 100 training steps: 0.3326989300006413, ID: 3200\n",
      "Training loss per 100 training steps: 0.3299185127208629, ID: 3300\n",
      "All files saved\n",
      "Training loss epoch: 0.3294467536198189\n",
      "Training accuracy epoch: 0.9012356576610877\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 0.11597251147031784, ID: 0\n",
      "Training loss per 100 training steps: 0.20033588172351519, ID: 100\n",
      "Training loss per 100 training steps: 0.19452159462464205, ID: 200\n",
      "Training loss per 100 training steps: 0.18277124110284412, ID: 300\n",
      "Training loss per 100 training steps: 0.1872808797583402, ID: 400\n",
      "Training loss per 100 training steps: 0.19249449538191532, ID: 500\n",
      "Training loss per 100 training steps: 0.19888217766620467, ID: 600\n",
      "Training loss per 100 training steps: 0.2030596001111582, ID: 700\n",
      "Training loss per 100 training steps: 0.20003506418093156, ID: 800\n",
      "Training loss per 100 training steps: 0.20110050701592055, ID: 900\n",
      "Training loss per 100 training steps: 0.20110593654925224, ID: 1000\n",
      "Training loss per 100 training steps: 0.19966075752547127, ID: 1100\n",
      "Training loss per 100 training steps: 0.19917216093270115, ID: 1200\n",
      "Training loss per 100 training steps: 0.19907204336830364, ID: 1300\n",
      "Training loss per 100 training steps: 0.1970881790799859, ID: 1400\n",
      "Training loss per 100 training steps: 0.1946204906801999, ID: 1500\n",
      "Training loss per 100 training steps: 0.19496793128991574, ID: 1600\n",
      "Training loss per 100 training steps: 0.1933445389002566, ID: 1700\n",
      "Training loss per 100 training steps: 0.19350435548456285, ID: 1800\n",
      "Training loss per 100 training steps: 0.1945084842793479, ID: 1900\n",
      "Training loss per 100 training steps: 0.19231402259976274, ID: 2000\n",
      "Training loss per 100 training steps: 0.19140171016050483, ID: 2100\n",
      "Training loss per 100 training steps: 0.1903209971459282, ID: 2200\n",
      "Training loss per 100 training steps: 0.18968642954477818, ID: 2300\n",
      "Training loss per 100 training steps: 0.1885238810231376, ID: 2400\n",
      "Training loss per 100 training steps: 0.18881564461676384, ID: 2500\n",
      "Training loss per 100 training steps: 0.18758299021844876, ID: 2600\n",
      "Training loss per 100 training steps: 0.1877869691174603, ID: 2700\n",
      "Training loss per 100 training steps: 0.188735818607591, ID: 2800\n",
      "Training loss per 100 training steps: 0.18854990609453126, ID: 2900\n",
      "Training loss per 100 training steps: 0.18895719366566138, ID: 3000\n",
      "Training loss per 100 training steps: 0.18884818458105854, ID: 3100\n",
      "Training loss per 100 training steps: 0.18875058298341066, ID: 3200\n",
      "Training loss per 100 training steps: 0.18793672496005465, ID: 3300\n",
      "All files saved\n",
      "Training loss epoch: 0.18798893288502416\n",
      "Training accuracy epoch: 0.9404034347023061\n"
     ]
    }
   ],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "# def train(epoch):\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "        \n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train();\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "#         labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "        someoutput = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss, tr_logits = someoutput[0], someoutput[1]        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}, ID: {idx}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "\n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    save_model(tk=tokenizer, ml = model, epk=epoch)\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "360fd14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 100 training steps: 0.18786422765764954, ID: 0\n",
      "Training loss per 100 training steps: 0.186547505759654, ID: 100\n",
      "Training loss per 100 training steps: 0.18549549811076793, ID: 200\n",
      "Training loss per 100 training steps: 0.18364159035215846, ID: 300\n",
      "Training loss per 100 training steps: 0.1825051229602512, ID: 400\n",
      "Training loss per 100 training steps: 0.18117876747319403, ID: 500\n",
      "Training loss per 100 training steps: 0.1800638874887725, ID: 600\n",
      "Training loss per 100 training steps: 0.17913265686496305, ID: 700\n",
      "Training loss per 100 training steps: 0.1778258933199844, ID: 800\n",
      "Training loss per 100 training steps: 0.17685511325283795, ID: 900\n",
      "Training loss per 100 training steps: 0.1757336467892481, ID: 1000\n",
      "Training loss per 100 training steps: 0.17470867696876052, ID: 1100\n",
      "Training loss per 100 training steps: 0.17390276988994408, ID: 1200\n",
      "Training loss per 100 training steps: 0.17301077696377992, ID: 1300\n",
      "Training loss per 100 training steps: 0.17167796911097266, ID: 1400\n",
      "Training loss per 100 training steps: 0.1710286794964875, ID: 1500\n",
      "Training loss per 100 training steps: 0.17062070998155632, ID: 1600\n",
      "Training loss per 100 training steps: 0.1699568082652546, ID: 1700\n",
      "Training loss per 100 training steps: 0.1700388508289881, ID: 1800\n",
      "Training loss per 100 training steps: 0.1692377515821119, ID: 1900\n",
      "Training loss per 100 training steps: 0.1688597190623643, ID: 2000\n",
      "Training loss per 100 training steps: 0.1684205855722007, ID: 2100\n",
      "Training loss per 100 training steps: 0.16787074829168808, ID: 2200\n",
      "Training loss per 100 training steps: 0.1682079155280975, ID: 2300\n",
      "Training loss per 100 training steps: 0.16797387071808098, ID: 2400\n",
      "Training loss per 100 training steps: 0.16762786766885168, ID: 2500\n",
      "Training loss per 100 training steps: 0.16699813757438076, ID: 2600\n",
      "Training loss per 100 training steps: 0.16666474708238896, ID: 2700\n",
      "Training loss per 100 training steps: 0.16606054094734762, ID: 2800\n",
      "Training loss per 100 training steps: 0.16585926601215875, ID: 2900\n",
      "Training loss per 100 training steps: 0.16557699555963115, ID: 3000\n",
      "Training loss per 100 training steps: 0.1653272441750797, ID: 3100\n",
      "Training loss per 100 training steps: 0.1643288703735987, ID: 3200\n",
      "Training loss per 100 training steps: 0.16391182005220153, ID: 3300\n",
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(training_loader):\n",
    "    ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "    mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "    labels = batch['labels'].to(device, dtype = torch.long)\n",
    "    #         labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "    someoutput = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "    loss, tr_logits = someoutput[0], someoutput[1]        \n",
    "    tr_loss += loss.item()\n",
    "\n",
    "    nb_tr_steps += 1\n",
    "    nb_tr_examples += labels.size(0)\n",
    "\n",
    "    if idx % 100==0:\n",
    "        loss_step = tr_loss/nb_tr_steps\n",
    "        print(f\"Training loss per 100 training steps: {loss_step}, ID: {idx}\")\n",
    "\n",
    "    # compute training accuracy\n",
    "    flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "    active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "\n",
    "    # only compute accuracy at active labels\n",
    "    active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "    #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "\n",
    "    labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "    predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "    tr_labels.extend(labels)\n",
    "    tr_preds.extend(predictions)\n",
    "\n",
    "    tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "    # gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(\n",
    "        parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "    )\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "save_model(tk=tokenizer, ml = model, epk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08ae99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval();\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            someoutput = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss, eval_logits = someoutput[0], someoutput[1]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac65736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.02338421903550625\n",
      "Validation loss per 100 evaluation steps: 0.2665409466764938\n",
      "Validation loss per 100 evaluation steps: 0.2996994985880148\n",
      "Validation loss per 100 evaluation steps: 0.31488958498145114\n",
      "Validation loss per 100 evaluation steps: 0.30534930842965025\n",
      "Validation loss per 100 evaluation steps: 0.29187828005164185\n",
      "Validation loss per 100 evaluation steps: 0.28675054938511785\n",
      "Validation loss per 100 evaluation steps: 0.2836471598551746\n",
      "Validation loss per 100 evaluation steps: 0.28850552096513327\n",
      "Validation loss per 100 evaluation steps: 0.29297508056138327\n",
      "Validation loss per 100 evaluation steps: 0.28448893748907095\n",
      "Validation loss per 100 evaluation steps: 0.2791695355682581\n",
      "Validation loss per 100 evaluation steps: 0.27426035704834806\n",
      "Validation loss per 100 evaluation steps: 0.27162374630618286\n",
      "Validation loss per 100 evaluation steps: 0.2660222150060522\n",
      "Validation loss per 100 evaluation steps: 0.26129283363122763\n",
      "Validation loss per 100 evaluation steps: 0.2634052986865006\n",
      "Validation loss per 100 evaluation steps: 0.26453676606302234\n",
      "Validation loss per 100 evaluation steps: 0.2635125440810094\n",
      "Validation loss per 100 evaluation steps: 0.26916586915352425\n",
      "Validation loss per 100 evaluation steps: 0.2661366064058202\n",
      "Validation loss per 100 evaluation steps: 0.26446045348731173\n",
      "Validation loss per 100 evaluation steps: 0.270047920580943\n",
      "Validation loss per 100 evaluation steps: 0.2740170510432848\n",
      "Validation loss per 100 evaluation steps: 0.2749867367145976\n",
      "Validation loss per 100 evaluation steps: 0.27196939608398185\n",
      "Validation loss per 100 evaluation steps: 0.2753977510304568\n",
      "Validation loss per 100 evaluation steps: 0.27636667595922315\n",
      "Validation loss per 100 evaluation steps: 0.2750919862428996\n",
      "Validation loss per 100 evaluation steps: 0.2727449038481209\n",
      "Validation loss per 100 evaluation steps: 0.2706181509550181\n",
      "Validation loss per 100 evaluation steps: 0.27288070608148574\n",
      "Validation loss per 100 evaluation steps: 0.2731608228974879\n",
      "Validation loss per 100 evaluation steps: 0.2776887155023622\n",
      "Validation Loss: 0.27602938046956405\n",
      "Validation Accuracy: 0.9155630698208567\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b2dd599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GVIS\\anaconda3\\envs\\ner\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      EVENTS       0.36      0.23      0.28        39\n",
      "  FACILITIES       0.46      0.44      0.45        66\n",
      "GPE-LOCATION       0.75      0.76      0.75       550\n",
      "       GROUP       0.00      0.00      0.00         4\n",
      "    LANGUAGE       0.56      0.62      0.59        24\n",
      "    LOCATION       0.50      0.46      0.48        37\n",
      "       MONEY       0.80      0.80      0.80        45\n",
      "ORGANIZATION       0.57      0.74      0.64       961\n",
      "      PERSON       0.77      0.79      0.78       171\n",
      "    PRODUCTS       0.73      0.73      0.73      2653\n",
      "  QUANTITIES       0.77      0.83      0.80       167\n",
      "\n",
      "   micro avg       0.69      0.73      0.71      4717\n",
      "   macro avg       0.57      0.58      0.57      4717\n",
      "weighted avg       0.70      0.73      0.71      4717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a80621c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Me', 'O'),\n",
       " ('llamo', 'O'),\n",
       " ('Mar', 'B-PERSON'),\n",
       " ('Jose', 'I-PERSON'),\n",
       " ('.', 'O'),\n",
       " ('Vivo', 'O'),\n",
       " ('cerca', 'O'),\n",
       " ('del', 'O'),\n",
       " ('Guardia', 'B-FACILITIES'),\n",
       " ('Civil', 'I-ORGANIZATION'),\n",
       " (',', 'O'),\n",
       " ('que', 'O'),\n",
       " ('es', 'O'),\n",
       " ('uno', 'O'),\n",
       " ('de', 'O'),\n",
       " ('los', 'O'),\n",
       " ('lugares', 'O'),\n",
       " ('más', 'O'),\n",
       " ('famosos', 'O'),\n",
       " ('de', 'O'),\n",
       " ('España', 'B-GPE-LOCATION'),\n",
       " ('.', 'O'),\n",
       " ('Ahi', 'O'),\n",
       " ('se', 'O'),\n",
       " ('venden', 'O'),\n",
       " ('grabanzo', 'B-PRODUCTS'),\n",
       " (',', 'O'),\n",
       " ('barras', 'B-PRODUCTS'),\n",
       " ('de', 'I-PRODUCTS'),\n",
       " ('chocolate', 'I-PRODUCTS'),\n",
       " ('y', 'O'),\n",
       " ('Coca-Cola', 'B-PRODUCTS'),\n",
       " ('.', 'O'),\n",
       " ('La', 'O'),\n",
       " ('gente', 'O'),\n",
       " ('habla', 'O'),\n",
       " ('principalmente', 'O'),\n",
       " ('español', 'B-LANGUAGE'),\n",
       " ('y', 'O'),\n",
       " ('francés', 'B-LANGUAGE'),\n",
       " ('.', 'O'),\n",
       " ('5', 'B-MONEY'),\n",
       " ('euros', 'I-MONEY'),\n",
       " ('¡Asistiremos', 'O'),\n",
       " ('al', 'O'),\n",
       " ('concierto', 'O'),\n",
       " ('de', 'O'),\n",
       " ('Wesam', 'B-EVENTS'),\n",
       " ('2023', 'I-EVENTS'),\n",
       " ('que', 'O'),\n",
       " ('sería', 'O'),\n",
       " ('increíble', 'O'),\n",
       " ('!', 'O'),\n",
       " ('.', 'O'),\n",
       " ('Vamos', 'O'),\n",
       " ('a', 'O'),\n",
       " ('quedarnos', 'O'),\n",
       " ('a', 'O'),\n",
       " ('las', 'O'),\n",
       " ('18:00', 'O'),\n",
       " ('de', 'O'),\n",
       " ('la', 'O'),\n",
       " ('noche', 'O'),\n",
       " ('!', 'O')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "text = \"Mañana iré a la empresa Alesida a ver la posibilidad de comprar un piso grande con ventanales gigantes. También puedes comprar un iPhone modelo año 2022\"\n",
    "text = 'Me llamo Mar Jose. Vivo cerca del Guardia Civil, que es uno de los lugares más famosos de España. ' \\\n",
    "       'Ahi se venden grabanzo, barras de chocolate y Coca-Cola. La gente habla principalmente español y francés. ' \\\n",
    "       '5 euros ¡Asistiremos al concierto de Wesam 2023 que sería increíble!. ' \\\n",
    "       'Vamos a quedarnos a las 18:00 de la noche!'\n",
    "\n",
    "inputs = tokenizer(nltk.word_tokenize(text),\n",
    "                    is_split_into_words=True, \n",
    "                    return_offsets_mapping=True, \n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    max_length=256,\n",
    "                    return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device, dtype = torch.long)\n",
    "mask = inputs[\"attention_mask\"].to(device, dtype = torch.long)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(ids, attention_mask=mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "  #only predictions on first word pieces are important\n",
    "  if mapping[0] == 0 and mapping[1] != 0:\n",
    "    prediction.append(token_pred[1])\n",
    "  else:\n",
    "    continue\n",
    "\n",
    "# print(sentence.split())\n",
    "# print(prediction)\n",
    "prd  = list(zip(nltk.word_tokenize(text), prediction))\n",
    "prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ded76c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "save_model(tk=tokenizer, ml=model, epk='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4f3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
