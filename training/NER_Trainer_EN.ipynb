{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450cf615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb#scrollTo=Eh3ckSO0YMZW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57827cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import pandas as pd \n",
    "\n",
    "df1 = pd.read_csv('./NER_DS_ENGLISH/TOR_ILLEGAL_ENG.CSV', encoding='utf-8')\n",
    "df2 = pd.read_csv('./NER_DS_ENGLISH/CONLL_ENG.csv', encoding='utf-8')\n",
    "df = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c7fca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22407"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df.sentence_idx.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa57333a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387804, 27348)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset:\n",
    "split_thresh = df['sentence_idx'].max() * 0.9\n",
    "df_train, df_valid = df[df['sentence_idx'] < split_thresh], df[df['sentence_idx'] >= split_thresh]\n",
    "len(df_train), len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85008fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = lambda s: [ [w,t] for w,t in zip(s[\"word\"].values.tolist(),s[\"tag\"].values.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97216cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_grouped = df_train.groupby(\"sentence_idx\").apply(agg_func)\n",
    "x_valid_grouped = df_valid.groupby(\"sentence_idx\").apply(agg_func)\n",
    "\n",
    "x_train_sentences = [[s[0] for s in sent] for sent in x_train_grouped.values]\n",
    "x_valid_sentences = [[s[0] for s in sent] for sent in x_valid_grouped.values]\n",
    "\n",
    "x_train_tags = [[t[1] for t in tag] for tag in x_train_grouped.values]\n",
    "x_valid_tags = [[t[1] for t in tag] for tag in x_valid_grouped.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f5ed977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list = df['tag'].unique()\n",
    "label_map = {label: i for i, label in enumerate(tag_list)}\n",
    "label_map_inv = {i: label for i, label in enumerate(tag_list)}\n",
    "num_labels = len(tag_list) + 1\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1a3a631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O         328511\n",
       "B-LOC      11242\n",
       "B-PER      10370\n",
       "B-ORG      10087\n",
       "B-DRG       9561\n",
       "I-DRG       8619\n",
       "I-PER       7110\n",
       "I-ORG       6372\n",
       "I-WEP       5753\n",
       "B-MISC      5222\n",
       "B-CUR       3911\n",
       "B-WEP       3038\n",
       "I-MISC      1966\n",
       "I-LOC       1793\n",
       "I-DAT       1291\n",
       "B-DAT        306\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of tags: {}\".format(len(df.tag.unique())))\n",
    "frequencies = df.tag.value_counts()\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c2bab6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DRG', 18180),\n",
       " ('PER', 17480),\n",
       " ('ORG', 16459),\n",
       " ('LOC', 13035),\n",
       " ('WEP', 8791),\n",
       " ('MISC', 7188),\n",
       " ('CUR', 3911),\n",
       " ('DAT', 1597)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {}\n",
    "for tag, count in zip(frequencies.index, frequencies):\n",
    "    if tag != \"O\":\n",
    "        if tag[2:] not in tags.keys():\n",
    "            tags[tag[2:]] = count\n",
    "        else:\n",
    "            tags[tag[2:]] += count\n",
    "    continue\n",
    "\n",
    "sorted(tags.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb584a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities_to_remove = [\"B-GROUP\", \"I-GROUP\",\n",
    "#                       \"B-NUMBERS_O\", \"I-NUMBERS_O\",\n",
    "#                       \"B-NORP\", \"I-NORP\",\n",
    "#                       \"B-LAW\", \"I-LAW\",\n",
    "#                       \"B-NUMBERS_C\", \"I-NUMBERS_C\",\n",
    "#                       \"B-DATE\", \"I-DATE\",\n",
    "#                       \"B-FACILITIES\", \"I-FACILITIES\", \n",
    "#                       \"B-ART\", \"I-ART\", \n",
    "#                       \"B-Time\", \"I-Time\", \n",
    "#                       \"B-GPE-LOCATION\", \"I-GPE-LOCATION\",\n",
    "#                      ]\n",
    "# df = df[~df.tag.isin(entities_to_remove)]\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0517856d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-DRG': 1,\n",
       " 'I-DRG': 2,\n",
       " 'B-CUR': 3,\n",
       " 'B-ORG': 4,\n",
       " 'I-ORG': 5,\n",
       " 'B-DAT': 6,\n",
       " 'I-DAT': 7,\n",
       " 'B-LOC': 8,\n",
       " 'B-PER': 9,\n",
       " 'I-PER': 10,\n",
       " 'I-LOC': 11,\n",
       " 'B-MISC': 12,\n",
       " 'I-MISC': 13,\n",
       " 'B-WEP': 14,\n",
       " 'I-WEP': 15}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids = {k: v for v, k in enumerate(df.tag.unique())}\n",
    "ids_to_labels = {v: k for v, k in enumerate(df.tag.unique())}\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bfa6bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100g</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Purple</td>\n",
       "      <td>B-DRG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Kush</td>\n",
       "      <td>I-DRG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>650</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>GBP</td>\n",
       "      <td>B-CUR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_idx    word    tag  Unnamed: 0\n",
       "0             0    100g      O         NaN\n",
       "1             0  Purple  B-DRG         NaN\n",
       "2             0    Kush  I-DRG         NaN\n",
       "3             0     650      O         NaN\n",
       "4             0     GBP  B-CUR         NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas has a very handy \"forward fill\" function to fill missing values based on the last upper non-nan value\n",
    "df = df.fillna(method='ffill')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "635dc44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100g</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100g Purple Kush 650 GBP 1.28 EU rejects Germa...</td>\n",
       "      <td>O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Purple</td>\n",
       "      <td>B-DRG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100g Purple Kush 650 GBP 1.28 EU rejects Germa...</td>\n",
       "      <td>O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Kush</td>\n",
       "      <td>I-DRG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100g Purple Kush 650 GBP 1.28 EU rejects Germa...</td>\n",
       "      <td>O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>650</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100g Purple Kush 650 GBP 1.28 EU rejects Germa...</td>\n",
       "      <td>O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>GBP</td>\n",
       "      <td>B-CUR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100g Purple Kush 650 GBP 1.28 EU rejects Germa...</td>\n",
       "      <td>O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_idx    word    tag  Unnamed: 0  \\\n",
       "0             0    100g      O         NaN   \n",
       "1             0  Purple  B-DRG         NaN   \n",
       "2             0    Kush  I-DRG         NaN   \n",
       "3             0     650      O         NaN   \n",
       "4             0     GBP  B-CUR         NaN   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  100g Purple Kush 650 GBP 1.28 EU rejects Germa...   \n",
       "1  100g Purple Kush 650 GBP 1.28 EU rejects Germa...   \n",
       "2  100g Purple Kush 650 GBP 1.28 EU rejects Germa...   \n",
       "3  100g Purple Kush 650 GBP 1.28 EU rejects Germa...   \n",
       "4  100g Purple Kush 650 GBP 1.28 EU rejects Germa...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...  \n",
       "1  O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...  \n",
       "2  O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...  \n",
       "3  O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...  \n",
       "4  O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create a new column called \"sentence\" which groups the words by sentence \n",
    "df['sentence'] = df[['sentence_idx','word','tag']].groupby(['sentence_idx'])['word'].transform(lambda x: ' '.join(x))\n",
    "# let's also create a new column called \"word_labels\" which groups the tags by sentence \n",
    "df['word_labels'] = df[['sentence_idx','word','tag']].groupby(['sentence_idx'])['tag'].transform(lambda x: ','.join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ad486c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100g Purple Kush 650 GBP 1.28 EU rejects Germa...</td>\n",
       "      <td>O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5g Banana Kush 0.071 X Peter Blackburn</td>\n",
       "      <td>O,B-DRG,I-DRG,O,O,B-PER,I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5g pure Cocaine 0.712 X BRUSSELS 22/08/1996</td>\n",
       "      <td>O,O,B-DRG,O,O,B-LOC,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Please use this link to go back to the top of ...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-ORG,I-ORG,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please use this link to go to the Front Page o...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,B-DRG,I-DRG,I-DRG,O,O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  100g Purple Kush 650 GBP 1.28 EU rejects Germa...   \n",
       "1             5g Banana Kush 0.071 X Peter Blackburn   \n",
       "2        5g pure Cocaine 0.712 X BRUSSELS 22/08/1996   \n",
       "3  Please use this link to go back to the top of ...   \n",
       "4  Please use this link to go to the Front Page o...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,B-DRG,I-DRG,O,B-CUR,O,B-ORG,O,B-MISC,O,O,O,B...  \n",
       "1                      O,B-DRG,I-DRG,O,O,B-PER,I-PER  \n",
       "2                              O,O,B-DRG,O,O,B-LOC,O  \n",
       "3  O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-ORG,I-ORG,O,O,O,...  \n",
       "4  O,O,O,O,O,O,O,O,O,O,O,O,B-DRG,I-DRG,I-DRG,O,O,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55014191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hidden split chat\n",
    "df['sentence'] = df['sentence'].str.replace('\\\\xa0','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5635a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21367"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfac5ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x.split()) for x in df.sentence.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de510fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da8a6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6618493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        sentence = self.data.sentence[index].strip().split()  \n",
    "        word_labels = self.data.word_labels[index].split(\",\") \n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                             is_split_into_words=True, \n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [labels_to_ids[label] for label in word_labels] \n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        \n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "          if mapping[0] == 0 and mapping[1] != 0:\n",
    "            # overwrite label\n",
    "            encoded_labels[idx] = labels[i]\n",
    "            i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "408d4244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (21367, 2)\n",
      "TRAIN Dataset: (17094, 2)\n",
      "TEST Dataset: (4273, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = df.sample(frac=train_size,random_state=200)\n",
    "test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46f11189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please use this link to go back to the top of ...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-ORG,I-ORG,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please use this link to go to the Front Page o...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,B-DRG,I-DRG,I-DRG,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please use this link to go to the Cocaine We d...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,B-DRG,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A systematic study has never been done , but p...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DRG,O,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Those beautiful and absolutely tasteless Blott...</td>\n",
       "      <td>O,O,O,O,O,B-DRG,O,O,O,B-DRG,I-DRG,I-DRG,O,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>PSV , well on the way to their 14th league tit...</td>\n",
       "      <td>B-ORG,O,O,O,O,O,O,O,O,O,O,O,O,B-ORG,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>, are one of the surprise packages of the seas...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>SOCCER - SPANISH FIRST DIVISION RESULT / STAND...</td>\n",
       "      <td>O,O,B-MISC,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>Jack Charlton 's relationship with the people ...</td>\n",
       "      <td>B-PER,I-PER,O,O,O,O,O,O,B-LOC,O,O,O,O,O,O,B-MI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>That is why this is so emotional a night for me ,</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4273 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     Please use this link to go back to the top of ...   \n",
       "1     Please use this link to go to the Front Page o...   \n",
       "2     Please use this link to go to the Cocaine We d...   \n",
       "3     A systematic study has never been done , but p...   \n",
       "4     Those beautiful and absolutely tasteless Blott...   \n",
       "...                                                 ...   \n",
       "4268  PSV , well on the way to their 14th league tit...   \n",
       "4269  , are one of the surprise packages of the seas...   \n",
       "4270  SOCCER - SPANISH FIRST DIVISION RESULT / STAND...   \n",
       "4271  Jack Charlton 's relationship with the people ...   \n",
       "4272  That is why this is so emotional a night for me ,   \n",
       "\n",
       "                                            word_labels  \n",
       "0     O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-ORG,I-ORG,O,O,O,...  \n",
       "1     O,O,O,O,O,O,O,O,O,O,O,O,B-DRG,I-DRG,I-DRG,O,O,...  \n",
       "2     O,O,O,O,O,O,O,O,B-DRG,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
       "3     O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DRG,O,O,O,O,...  \n",
       "4     O,O,O,O,O,B-DRG,O,O,O,B-DRG,I-DRG,I-DRG,O,O,O,...  \n",
       "...                                                 ...  \n",
       "4268  B-ORG,O,O,O,O,O,O,O,O,O,O,O,O,B-ORG,O,O,O,O,O,O,O  \n",
       "4269                              O,O,O,O,O,O,O,O,O,O,O  \n",
       "4270                             O,O,B-MISC,O,O,O,O,O,O  \n",
       "4271  B-PER,I-PER,O,O,O,O,O,O,B-LOC,O,O,O,O,O,O,B-MI...  \n",
       "4272                            O,O,O,O,O,O,O,O,O,O,O,O  \n",
       "\n",
       "[4273 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a472628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.iloc[13518].word_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dce899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[80][\"input_ids\"]), training_set[80][\"labels\"]):\n",
    "    # print('{0:10}  {1}'.format(token, label))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d7933b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1682c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
    "\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cb1b57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))\n",
    "# model.to(device)\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", \n",
    "                                                   num_labels=len(labels_to_ids), \n",
    "                                                   ignore_mismatched_sizes=True)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2256380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = training_set[0]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)\n",
    "# labels = labels.type(torch.LongTensor)\n",
    "\n",
    "input_ids = input_ids.to(device, dtype = torch.long)\n",
    "attention_mask = attention_mask.to(device, dtype = torch.long)\n",
    "labels = labels.to(device, dtype = torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f80fe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7571, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "809c141d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0e585f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "290b4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(tk, ml, epk):    \n",
    "\n",
    "    directory = \"./bert-base-cased_en\"\n",
    "    \n",
    "    p  = os.path.join(directory,f\"epoch_{str(epk)}\")\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    \n",
    "    # save vocabulary of the tokenizer\n",
    "    tk.save_vocabulary(p)\n",
    "    \n",
    "    # save the model weights and its configuration file\n",
    "    ml.save_pretrained(p)\n",
    "    print('All files saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3889b117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 2.7239410877227783, ID: 0\n",
      "Training loss per 100 training steps: 1.083782142341727, ID: 100\n",
      "Training loss per 100 training steps: 0.8172734546572414, ID: 200\n",
      "Training loss per 100 training steps: 0.6689168287571087, ID: 300\n",
      "Training loss per 100 training steps: 0.5725393748640122, ID: 400\n",
      "Training loss per 100 training steps: 0.5026201060759331, ID: 500\n",
      "Training loss per 100 training steps: 0.4487009208506832, ID: 600\n",
      "Training loss per 100 training steps: 0.40928675053101804, ID: 700\n",
      "Training loss per 100 training steps: 0.3777172126703792, ID: 800\n",
      "Training loss per 100 training steps: 0.35252156773877724, ID: 900\n",
      "Training loss per 100 training steps: 0.3321089236908681, ID: 1000\n",
      "Training loss per 100 training steps: 0.315500746206207, ID: 1100\n",
      "Training loss per 100 training steps: 0.29999685773143636, ID: 1200\n",
      "Training loss per 100 training steps: 0.2878770898636731, ID: 1300\n",
      "Training loss per 100 training steps: 0.27688138956652103, ID: 1400\n",
      "Training loss per 100 training steps: 0.2664180435060333, ID: 1500\n",
      "Training loss per 100 training steps: 0.2569081801193988, ID: 1600\n",
      "Training loss per 100 training steps: 0.2488183535171179, ID: 1700\n",
      "Training loss per 100 training steps: 0.2411572329921318, ID: 1800\n",
      "Training loss per 100 training steps: 0.23417243384449954, ID: 1900\n",
      "Training loss per 100 training steps: 0.22726792882339492, ID: 2000\n",
      "Training loss per 100 training steps: 0.221382271878994, ID: 2100\n",
      "All files saved\n",
      "Training loss epoch: 0.21935428832023535\n",
      "Training accuracy epoch: 0.9411404519937797\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 0.03635711967945099, ID: 0\n",
      "Training loss per 100 training steps: 0.08758887208278963, ID: 100\n",
      "Training loss per 100 training steps: 0.0786499887130302, ID: 200\n",
      "Training loss per 100 training steps: 0.07745986754860593, ID: 300\n",
      "Training loss per 100 training steps: 0.07809738908195288, ID: 400\n",
      "Training loss per 100 training steps: 0.07821313742392078, ID: 500\n",
      "Training loss per 100 training steps: 0.07747658168759117, ID: 600\n",
      "Training loss per 100 training steps: 0.07677521635366394, ID: 700\n",
      "Training loss per 100 training steps: 0.07632083753961023, ID: 800\n",
      "Training loss per 100 training steps: 0.07504052211676251, ID: 900\n",
      "Training loss per 100 training steps: 0.0752796316482164, ID: 1000\n",
      "Training loss per 100 training steps: 0.0750107682782896, ID: 1100\n",
      "Training loss per 100 training steps: 0.07431439525074791, ID: 1200\n",
      "Training loss per 100 training steps: 0.07374331086400114, ID: 1300\n",
      "Training loss per 100 training steps: 0.07344099493706592, ID: 1400\n",
      "Training loss per 100 training steps: 0.07372650113372475, ID: 1500\n",
      "Training loss per 100 training steps: 0.07371193582771159, ID: 1600\n",
      "Training loss per 100 training steps: 0.07307670923418856, ID: 1700\n",
      "Training loss per 100 training steps: 0.07267608078666443, ID: 1800\n",
      "Training loss per 100 training steps: 0.0722789287941064, ID: 1900\n",
      "Training loss per 100 training steps: 0.07170399151276331, ID: 2000\n",
      "Training loss per 100 training steps: 0.07109085405636614, ID: 2100\n",
      "All files saved\n",
      "Training loss epoch: 0.07094051477547898\n",
      "Training accuracy epoch: 0.9792094430456834\n",
      "Training epoch: 3\n",
      "Training loss per 100 training steps: 0.015523085370659828, ID: 0\n",
      "Training loss per 100 training steps: 0.04653628836017065, ID: 100\n",
      "Training loss per 100 training steps: 0.045073237890419345, ID: 200\n",
      "Training loss per 100 training steps: 0.045884211650938166, ID: 300\n",
      "Training loss per 100 training steps: 0.0466750713301581, ID: 400\n",
      "Training loss per 100 training steps: 0.04847943225290864, ID: 500\n",
      "Training loss per 100 training steps: 0.04727954867574078, ID: 600\n",
      "Training loss per 100 training steps: 0.04743122400768426, ID: 700\n",
      "Training loss per 100 training steps: 0.04765289722428484, ID: 800\n",
      "Training loss per 100 training steps: 0.047025226344319306, ID: 900\n",
      "Training loss per 100 training steps: 0.04679290686357677, ID: 1000\n",
      "Training loss per 100 training steps: 0.04633190988600643, ID: 1100\n",
      "Training loss per 100 training steps: 0.04596565453355805, ID: 1200\n",
      "Training loss per 100 training steps: 0.04546732740266625, ID: 1300\n",
      "Training loss per 100 training steps: 0.04524937078137706, ID: 1400\n",
      "Training loss per 100 training steps: 0.04458108481674639, ID: 1500\n",
      "Training loss per 100 training steps: 0.044222456385298044, ID: 1600\n",
      "Training loss per 100 training steps: 0.04401262839368437, ID: 1700\n",
      "Training loss per 100 training steps: 0.043892529241419835, ID: 1800\n",
      "Training loss per 100 training steps: 0.043442072200659675, ID: 1900\n",
      "Training loss per 100 training steps: 0.04360299223895138, ID: 2000\n",
      "Training loss per 100 training steps: 0.04355624495740729, ID: 2100\n",
      "All files saved\n",
      "Training loss epoch: 0.043456223043819055\n",
      "Training accuracy epoch: 0.9870606126438479\n",
      "Training epoch: 4\n",
      "Training loss per 100 training steps: 0.017252933233976364, ID: 0\n",
      "Training loss per 100 training steps: 0.025385839850780103, ID: 100\n",
      "Training loss per 100 training steps: 0.028302789949557157, ID: 200\n",
      "Training loss per 100 training steps: 0.028672009396700938, ID: 300\n",
      "Training loss per 100 training steps: 0.029475218777839755, ID: 400\n",
      "Training loss per 100 training steps: 0.029283997542345654, ID: 500\n",
      "Training loss per 100 training steps: 0.030215742737972975, ID: 600\n",
      "Training loss per 100 training steps: 0.03149027729876268, ID: 700\n",
      "Training loss per 100 training steps: 0.03186970279734005, ID: 800\n",
      "Training loss per 100 training steps: 0.03145098290858391, ID: 900\n",
      "Training loss per 100 training steps: 0.03125565368544913, ID: 1000\n",
      "Training loss per 100 training steps: 0.031055565030617703, ID: 1100\n",
      "Training loss per 100 training steps: 0.03063008211350315, ID: 1200\n",
      "Training loss per 100 training steps: 0.03069951116108478, ID: 1300\n",
      "Training loss per 100 training steps: 0.030878861047496035, ID: 1400\n",
      "Training loss per 100 training steps: 0.030770791792361963, ID: 1500\n",
      "Training loss per 100 training steps: 0.030539082950741682, ID: 1600\n",
      "Training loss per 100 training steps: 0.03066553069495003, ID: 1700\n",
      "Training loss per 100 training steps: 0.03047876564192062, ID: 1800\n",
      "Training loss per 100 training steps: 0.03024550688478494, ID: 1900\n",
      "Training loss per 100 training steps: 0.029997547105446214, ID: 2000\n",
      "Training loss per 100 training steps: 0.030153037142268968, ID: 2100\n",
      "All files saved\n",
      "Training loss epoch: 0.03003289913056439\n",
      "Training accuracy epoch: 0.9908897730247416\n",
      "Training epoch: 5\n",
      "Training loss per 100 training steps: 0.09312278777360916, ID: 0\n",
      "Training loss per 100 training steps: 0.02158968705144517, ID: 100\n",
      "Training loss per 100 training steps: 0.020925462991925912, ID: 200\n",
      "Training loss per 100 training steps: 0.020669314200623252, ID: 300\n",
      "Training loss per 100 training steps: 0.020435326179649747, ID: 400\n",
      "Training loss per 100 training steps: 0.021169221896746526, ID: 500\n",
      "Training loss per 100 training steps: 0.020792400751628227, ID: 600\n",
      "Training loss per 100 training steps: 0.02083855443682253, ID: 700\n",
      "Training loss per 100 training steps: 0.020518909814315832, ID: 800\n",
      "Training loss per 100 training steps: 0.020934072028422783, ID: 900\n",
      "Training loss per 100 training steps: 0.02146885417007886, ID: 1000\n",
      "Training loss per 100 training steps: 0.02164339110459629, ID: 1100\n",
      "Training loss per 100 training steps: 0.021574162834694804, ID: 1200\n",
      "Training loss per 100 training steps: 0.02162741202919502, ID: 1300\n",
      "Training loss per 100 training steps: 0.02187504070319636, ID: 1400\n",
      "Training loss per 100 training steps: 0.021585235933418465, ID: 1500\n",
      "Training loss per 100 training steps: 0.02163184025483907, ID: 1600\n",
      "Training loss per 100 training steps: 0.021788442842037483, ID: 1700\n",
      "Training loss per 100 training steps: 0.02187845040741868, ID: 1800\n",
      "Training loss per 100 training steps: 0.022060668182431076, ID: 1900\n",
      "Training loss per 100 training steps: 0.022108844087408763, ID: 2000\n",
      "Training loss per 100 training steps: 0.02204856564842312, ID: 2100\n",
      "All files saved\n",
      "Training loss epoch: 0.021933660282069514\n",
      "Training accuracy epoch: 0.9931528593544889\n"
     ]
    }
   ],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "# def train(epoch):\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "        \n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train();\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "#         labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "        someoutput = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss, tr_logits = someoutput[0], someoutput[1]        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}, ID: {idx}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "\n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    save_model(tk=tokenizer, ml = model, epk=epoch)\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08ae99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval();\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            someoutput = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss, eval_logits = someoutput[0], someoutput[1]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac65736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.04244834929704666\n",
      "Validation loss per 100 evaluation steps: 0.04803369089718712\n",
      "Validation loss per 100 evaluation steps: 0.042919962155403606\n",
      "Validation loss per 100 evaluation steps: 0.04803119140858545\n",
      "Validation loss per 100 evaluation steps: 0.05132356928434296\n",
      "Validation loss per 100 evaluation steps: 0.054814144338526966\n",
      "Validation loss per 100 evaluation steps: 0.05314136689419671\n",
      "Validation loss per 100 evaluation steps: 0.051927018797485056\n",
      "Validation loss per 100 evaluation steps: 0.052388068023410075\n",
      "Validation loss per 100 evaluation steps: 0.053658724621244006\n",
      "Validation loss per 100 evaluation steps: 0.05165153940899407\n",
      "Validation loss per 100 evaluation steps: 0.05078425480106596\n",
      "Validation loss per 100 evaluation steps: 0.05241333129485219\n",
      "Validation loss per 100 evaluation steps: 0.053097474464233414\n",
      "Validation loss per 100 evaluation steps: 0.05430578602204621\n",
      "Validation loss per 100 evaluation steps: 0.05479619081622739\n",
      "Validation loss per 100 evaluation steps: 0.05626124187620234\n",
      "Validation loss per 100 evaluation steps: 0.05699924726614635\n",
      "Validation loss per 100 evaluation steps: 0.056614235184821074\n",
      "Validation loss per 100 evaluation steps: 0.057134960040460724\n",
      "Validation loss per 100 evaluation steps: 0.055550291076965796\n",
      "Validation loss per 100 evaluation steps: 0.05527992255062255\n",
      "Validation Loss: 0.054782139030469516\n",
      "Validation Accuracy: 0.9867485548296981\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5e78cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CUR       1.00      1.00      1.00       778\n",
      "         DAT       0.90      0.92      0.91        79\n",
      "         DRG       0.89      0.90      0.90      1905\n",
      "         LOC       0.96      0.96      0.96      2210\n",
      "        MISC       0.89      0.89      0.89      1069\n",
      "         ORG       0.91      0.94      0.92      1966\n",
      "         PER       0.97      0.97      0.97      2078\n",
      "         WEP       0.89      0.94      0.92       632\n",
      "\n",
      "   micro avg       0.93      0.94      0.94     10717\n",
      "   macro avg       0.93      0.94      0.93     10717\n",
      "weighted avg       0.93      0.94      0.94     10717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3905d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install seqeval==0.0.12\n",
    "# ! pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a80621c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'O'),\n",
       " ('have', 'O'),\n",
       " ('been', 'O'),\n",
       " ('working', 'O'),\n",
       " ('in', 'O'),\n",
       " ('Spain', 'B-LOC'),\n",
       " ('for', 'O'),\n",
       " ('more', 'O'),\n",
       " ('than', 'O'),\n",
       " ('10', 'O'),\n",
       " ('years', 'O'),\n",
       " ('and', 'O'),\n",
       " ('I', 'O'),\n",
       " ('met', 'O'),\n",
       " ('Sara', 'B-PER'),\n",
       " ('and', 'O'),\n",
       " ('Khaled,', 'B-PER'),\n",
       " ('I', 'O'),\n",
       " ('love', 'O'),\n",
       " ('them.', 'O'),\n",
       " ('Also', 'O'),\n",
       " ('I', 'O'),\n",
       " ('was', 'O'),\n",
       " ('in', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Guardia', 'B-ORG'),\n",
       " ('Civil', 'I-ORG'),\n",
       " ('office', 'O'),\n",
       " ('with', 'O'),\n",
       " ('Enrique', 'B-PER')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I have been working in Spain for more than 10 years and I met Sara and Khaled, I love them. Also I was in the Guardia Civil office with Enrique\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(sentence.split(),\n",
    "                    is_split_into_words=True, \n",
    "                    return_offsets_mapping=True, \n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    max_length=MAX_LEN,\n",
    "                    return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device, dtype = torch.long)\n",
    "mask = inputs[\"attention_mask\"].to(device, dtype = torch.long)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(ids, attention_mask=mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "  #only predictions on first word pieces are important\n",
    "  if mapping[0] == 0 and mapping[1] != 0:\n",
    "    prediction.append(token_pred[1])\n",
    "  else:\n",
    "    continue\n",
    "\n",
    "# print(sentence.split())\n",
    "# print(prediction)\n",
    "prd  = list(zip(sentence.split(), prediction))\n",
    "prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efc91345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# directory = \"./bert-base-uncased-3epk\"\n",
    "\n",
    "# if not os.path.exists(directory):\n",
    "#     os.makedirs(directory)\n",
    "\n",
    "# # save vocabulary of the tokenizer\n",
    "# tokenizer.save_vocabulary(directory)\n",
    "# # save the model weights and its configuration file\n",
    "# model.save_pretrained(directory)\n",
    "# print('All files saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "413e0746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded76c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
